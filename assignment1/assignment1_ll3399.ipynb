{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 for DS-GA 1011\n",
    "#### Liangzhi Li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and split train data into training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readPath(path):\n",
    "    texts = []\n",
    "    for file in os.listdir(path):\n",
    "        filename = path+file\n",
    "        with open(filename) as f:\n",
    "            texts.append(f.read())\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_pos = '/Users/amberli/Downloads/CDS/fall 2018/nlp/aclImdb/train/pos/'\n",
    "path_neg = '/Users/amberli/Downloads/CDS/fall 2018/nlp/aclImdb/train/neg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = []\n",
    "train_data.extend(readPath(path_pos))\n",
    "train_data.extend(readPath(path_neg))\n",
    "train_target = []\n",
    "train_target.extend([1]*12500)\n",
    "train_target.extend([0]*12500)\n",
    "\n",
    "train_data, val_data, train_target, val_target = train_test_split(\n",
    "    train_data, train_target, test_size=0.2, random_state=2018)\n",
    "\n",
    "test_data = [] \n",
    "test_data.extend(readPath(path_pos))\n",
    "test_data.extend(readPath(path_neg))\n",
    "test_target = []\n",
    "test_target.extend([1]*12500)\n",
    "test_target.extend([0]*12500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')  # 'en_core_web_sm': English Vocabulary, syntax, entities\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    sent = sent.replace('<br />','')\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]  # takenize the sentence and change to lowercase\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in tqdm_notebook(dataset):\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import pickle as pkl\n",
    "\n",
    "## tokenization scheme 1: lower case + remove punctuation:\n",
    "def lower_case_remove_punc(parsed):\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "## scheme 2: lower case + remove punctuation + remove line breakers '\\n'\n",
    "def lower_case_remove_punc_line_break(parsed):\n",
    "    a = a.text.replace('<br />','')\n",
    "    a = tokenizer(a)\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "## scheme 3: without lower case and do not remove anything\n",
    "def without_tokenization(parsed):\n",
    "    return [token.text for token in parsed]\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "\n",
    "    for sample in tqdm_notebook(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "        tokens = lower_case_remove_punc(sample)\n",
    "        #tokens = without_tokenization(sample)\n",
    "        #tokens = lower_case_remove_punc_line_break(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6dac2df9d54a278abc653c76d1f838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2182c065b36549e28b6ccf6be16385ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc45c2993c9d4ddbaf8ffc77c40ec66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "from tqdm import tqdm\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens.p\", \"wb\"))\n",
    "\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open tokenized files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert word to ngrams\n",
    "def word2ngram(data,n):\n",
    "    ngram_list=[]\n",
    "    for sentence in data:\n",
    "        ngram = [tuple(sentence[i] for i in range(i,i+n)) for i in range(len(sentence)-n+1)]\n",
    "        ngram_list.append(ngram)\n",
    "    return ngram_list\n",
    "\n",
    "def word2ngram_all_tokens(data,n):\n",
    "    ngram_list=[]\n",
    "    for i in range(len(data)-n+1):\n",
    "        ngram = tuple(data[i] for i in range(i,i+n))\n",
    "        ngram_list.append(ngram)\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2bigram(data):\n",
    "    ngram_list=[]\n",
    "    for sentence in data:\n",
    "        ngram = [tuple(sentence[i] for i in range(i,i+2)) for i in range(len(sentence)-2+1)]\n",
    "        for i in range(len(sentence)):\n",
    "            ngram.append(sentence[i])\n",
    "        ngram_list.append(ngram)\n",
    "    return ngram_list\n",
    "\n",
    "def word2bigram_all_tokens(data):\n",
    "    ngram_list=[]\n",
    "    for i in range(len(data)-2+1):\n",
    "        ngram_list.append(tuple(data[i] for i in range(i,i+2)))\n",
    "        ngram_list.append(data[i])\n",
    "    return ngram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train_tokens = word2ngram_all_tokens(all_train_tokens,2)\n",
    "train_data_tokens = word2ngram(train_data_tokens,2)\n",
    "val_data_tokens = word2ngram(val_data_tokens,2)\n",
    "test_data_tokens = word2ngram(test_data_tokens,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_tokens = word2bigram_all_tokens(all_train_tokens)\n",
    "train_data_tokens = word2bigram(train_data_tokens)\n",
    "val_data_tokens = word2bigram(val_data_tokens)\n",
    "test_data_tokens = word2bigram(test_data_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the vocabulary of most common N tokens in the training set.\n",
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 20000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))    # select most commen 10000 token\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 12037 ; token cooler\n",
      "Token cooler; token id 12037\n"
     ]
    }
   ],
   "source": [
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pytorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "## data loader\n",
    "\n",
    "class IMDB_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def IMDB_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataset = IMDB_Dataset(train_data_indices, train_target)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = IMDB_Dataset(val_data_indices, val_target)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = IMDB_Dataset(test_data_indices, test_target)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=IMDB_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "#for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#    print (data)\n",
    "#    print (labels)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define bag-of-ngram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfNgram(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfNgram, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,20)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfNgram(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 78.78, loss: 0.5432278513908386\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 84.2, loss: 0.5663700103759766\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 86.08, loss: 0.3199162185192108\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 85.9, loss: 0.2217552810907364\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 87.2, loss: 0.2655215263366699\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 87.9, loss: 0.2848219871520996\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 87.76, loss: 0.20186670124530792\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 87.52, loss: 0.15709614753723145\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 87.66, loss: 0.24893170595169067\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 87.82, loss: 0.29584747552871704\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 87.28, loss: 0.28986629843711853\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 87.16, loss: 0.4625698924064636\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 87.32, loss: 0.13045331835746765\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 87.2, loss: 0.039299026131629944\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 87.04, loss: 0.041895151138305664\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 85.28, loss: 0.09397880733013153\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 87.06, loss: 0.11873167008161545\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 85.9, loss: 0.10902006924152374\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 86.18, loss: 0.03635045886039734\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 86.2, loss: 0.06968270242214203\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 86.12, loss: 0.032688021659851074\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 86.16, loss: 0.0686223953962326\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 86.0, loss: 0.0174332857131958\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 86.12, loss: 0.1353115439414978\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 85.96, loss: 0.03901846706867218\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 85.9, loss: 0.0390314906835556\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 86.02, loss: 0.024075329303741455\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 86.26, loss: 0.009025022387504578\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 85.4, loss: 0.0073076337575912476\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 85.8, loss: 0.02348846197128296\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 85.64, loss: 0.00435890257358551\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 85.3, loss: 0.009532108902931213\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 85.56, loss: 0.008909136056900024\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 85.56, loss: 0.005540400743484497\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 85.56, loss: 0.0024238675832748413\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 85.0, loss: 0.06043832004070282\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 85.7, loss: 0.0034337639808654785\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 85.64, loss: 0.01769927144050598\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 85.52, loss: 0.0011312365531921387\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 85.74, loss: 0.0026830583810806274\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 85.54, loss: 0.0007351934909820557\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 85.24, loss: 0.00693298876285553\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 85.32, loss: 0.004280552268028259\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 85.58, loss: 0.0011608153581619263\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 85.82, loss: 0.0007265657186508179\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 85.68, loss: 0.006077706813812256\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 85.46, loss: 0.009845629334449768\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 85.56, loss: 0.003169223666191101\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 85.34, loss: 0.0022010356187820435\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 85.48, loss: 0.0016905814409255981\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 85.46, loss: 0.0011795908212661743\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 85.54, loss: 0.000434190034866333\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 85.42, loss: 0.0005100518465042114\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 85.48, loss: 0.00027558207511901855\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 85.5, loss: 0.00023543834686279297\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 85.38, loss: 0.000390470027923584\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 85.4, loss: 0.0008409470319747925\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 85.44, loss: 0.0006663352251052856\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 85.4, loss: 0.00021664798259735107\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 85.48, loss: 0.00014528632164001465\n",
      "max validation accuracy: 87.9\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "#criterion = torch.nn.NLLLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "acc = []\n",
    "Loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    #print('learning rate: {}'.format(learning_rate))\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #loss_r += loss.item()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            acc.append(val_acc)\n",
    "            Loss.append(loss)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, loss: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc, loss))\n",
    "    #learning_rate -= 0.02   # linearly annealing of learning rate\n",
    "print('max validation accuracy: {}'.format(max(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Train Acc 100.0\n",
      "Val Acc 85.58\n",
      "Test Acc 97.116\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Train Acc {}\".format(test_model(train_loader, model)))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 3 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "#criterion = torch.nn.NLLLoss()  \n",
    "\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "#lambda1 = lambda epoch: epoch // 1\n",
    "#lambda2 = lambda epoch: 0.95 ** epoch\n",
    "#scheduler = LambdaLR(optimizer, lr_lambda=[lambda1])\n",
    "\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/625], Validation Acc: 73.48, loss: 0.4332491159439087\n",
      "Epoch: [1/10], Step: [201/625], Validation Acc: 84.34, loss: 0.4545050859451294\n",
      "Epoch: [1/10], Step: [301/625], Validation Acc: 84.66, loss: 0.36025264859199524\n",
      "Epoch: [1/10], Step: [401/625], Validation Acc: 86.78, loss: 0.24696289002895355\n",
      "Epoch: [1/10], Step: [501/625], Validation Acc: 87.46, loss: 0.3713677227497101\n",
      "Epoch: [1/10], Step: [601/625], Validation Acc: 86.84, loss: 0.32649579644203186\n",
      "Epoch: [2/10], Step: [101/625], Validation Acc: 87.86, loss: 0.13408571481704712\n",
      "Epoch: [2/10], Step: [201/625], Validation Acc: 86.58, loss: 0.21281513571739197\n",
      "Epoch: [2/10], Step: [301/625], Validation Acc: 87.72, loss: 0.2561343312263489\n",
      "Epoch: [2/10], Step: [401/625], Validation Acc: 86.86, loss: 0.11022761464118958\n",
      "Epoch: [2/10], Step: [501/625], Validation Acc: 87.3, loss: 0.20731139183044434\n",
      "Epoch: [2/10], Step: [601/625], Validation Acc: 87.5, loss: 0.134582981467247\n",
      "Epoch: [3/10], Step: [101/625], Validation Acc: 86.44, loss: 0.08220186829566956\n",
      "Epoch: [3/10], Step: [201/625], Validation Acc: 87.32, loss: 0.032923176884651184\n",
      "Epoch: [3/10], Step: [301/625], Validation Acc: 87.1, loss: 0.15639157593250275\n",
      "Epoch: [3/10], Step: [401/625], Validation Acc: 86.58, loss: 0.050560228526592255\n",
      "Epoch: [3/10], Step: [501/625], Validation Acc: 86.5, loss: 0.16777832806110382\n",
      "Epoch: [3/10], Step: [601/625], Validation Acc: 86.12, loss: 0.09163297712802887\n",
      "Epoch: [4/10], Step: [101/625], Validation Acc: 86.94, loss: 0.10834736377000809\n",
      "Epoch: [4/10], Step: [201/625], Validation Acc: 86.52, loss: 0.02631668746471405\n",
      "Epoch: [4/10], Step: [301/625], Validation Acc: 87.02, loss: 0.05867311358451843\n",
      "Epoch: [4/10], Step: [401/625], Validation Acc: 86.52, loss: 0.06059488654136658\n",
      "Epoch: [4/10], Step: [501/625], Validation Acc: 85.82, loss: 0.11023496836423874\n",
      "Epoch: [4/10], Step: [601/625], Validation Acc: 86.2, loss: 0.041234686970710754\n",
      "Epoch: [5/10], Step: [101/625], Validation Acc: 86.32, loss: 0.025763168931007385\n",
      "Epoch: [5/10], Step: [201/625], Validation Acc: 86.24, loss: 0.01413048803806305\n",
      "Epoch: [5/10], Step: [301/625], Validation Acc: 86.16, loss: 0.004861682653427124\n",
      "Epoch: [5/10], Step: [401/625], Validation Acc: 85.96, loss: 0.022758275270462036\n",
      "Epoch: [5/10], Step: [501/625], Validation Acc: 85.62, loss: 0.026669085025787354\n",
      "Epoch: [5/10], Step: [601/625], Validation Acc: 85.68, loss: 0.02740311622619629\n",
      "Epoch: [6/10], Step: [101/625], Validation Acc: 85.98, loss: 0.02928149700164795\n",
      "Epoch: [6/10], Step: [201/625], Validation Acc: 86.06, loss: 0.006258547306060791\n",
      "Epoch: [6/10], Step: [301/625], Validation Acc: 85.76, loss: 0.005288079380989075\n",
      "Epoch: [6/10], Step: [401/625], Validation Acc: 85.56, loss: 0.01873031258583069\n",
      "Epoch: [6/10], Step: [501/625], Validation Acc: 85.78, loss: 0.00817038118839264\n",
      "Epoch: [6/10], Step: [601/625], Validation Acc: 85.7, loss: 0.011735960841178894\n",
      "Epoch: [7/10], Step: [101/625], Validation Acc: 85.78, loss: 0.008928343653678894\n",
      "Epoch: [7/10], Step: [201/625], Validation Acc: 85.84, loss: 0.006299763917922974\n",
      "Epoch: [7/10], Step: [301/625], Validation Acc: 85.94, loss: 0.002445518970489502\n",
      "Epoch: [7/10], Step: [401/625], Validation Acc: 85.6, loss: 0.002297922968864441\n",
      "Epoch: [7/10], Step: [501/625], Validation Acc: 85.34, loss: 0.001181185245513916\n",
      "Epoch: [7/10], Step: [601/625], Validation Acc: 85.74, loss: 0.012119114398956299\n",
      "Epoch: [8/10], Step: [101/625], Validation Acc: 85.76, loss: 0.0007635802030563354\n",
      "Epoch: [8/10], Step: [201/625], Validation Acc: 85.88, loss: 0.0014678984880447388\n",
      "Epoch: [8/10], Step: [301/625], Validation Acc: 85.74, loss: 0.0027311891317367554\n",
      "Epoch: [8/10], Step: [401/625], Validation Acc: 85.76, loss: 0.0005255341529846191\n",
      "Epoch: [8/10], Step: [501/625], Validation Acc: 85.86, loss: 0.004653424024581909\n",
      "Epoch: [8/10], Step: [601/625], Validation Acc: 85.66, loss: 0.0008735507726669312\n",
      "Epoch: [9/10], Step: [101/625], Validation Acc: 85.7, loss: 0.0008231997489929199\n",
      "Epoch: [9/10], Step: [201/625], Validation Acc: 85.66, loss: 0.0009656250476837158\n",
      "Epoch: [9/10], Step: [301/625], Validation Acc: 85.7, loss: 0.000648990273475647\n",
      "Epoch: [9/10], Step: [401/625], Validation Acc: 85.7, loss: 0.0007006824016571045\n",
      "Epoch: [9/10], Step: [501/625], Validation Acc: 85.86, loss: 0.00024199485778808594\n",
      "Epoch: [9/10], Step: [601/625], Validation Acc: 85.54, loss: 0.0004028826951980591\n",
      "Epoch: [10/10], Step: [101/625], Validation Acc: 85.66, loss: 0.0006327778100967407\n",
      "Epoch: [10/10], Step: [201/625], Validation Acc: 85.56, loss: 0.0005835294723510742\n",
      "Epoch: [10/10], Step: [301/625], Validation Acc: 85.66, loss: 0.000523105263710022\n",
      "Epoch: [10/10], Step: [401/625], Validation Acc: 85.7, loss: 0.0003625601530075073\n",
      "Epoch: [10/10], Step: [501/625], Validation Acc: 85.82, loss: 0.0007117986679077148\n",
      "Epoch: [10/10], Step: [601/625], Validation Acc: 85.6, loss: 0.0003198087215423584\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 10 # number epoch to train\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "#criterion = torch.nn.NLLLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "ind = []\n",
    "acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_acc = []\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        #scheduler.step(loss)\n",
    "        # loss_r += loss.item()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            epoch_acc.append(val_acc)\n",
    "            #ind.append(i)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, loss: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc, loss))\n",
    "    acc.append(epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model_2(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i=0\n",
    "    pred = []\n",
    "    label = []\n",
    "    DATA = []\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        pred.append(predicted.view_as(labels))\n",
    "        DATA.append(data)\n",
    "        label.append(labels)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return DATA, pred,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmYVOWZ9/Hvj26gmx20QWhAaEAR\n0Yi2u4JGHU0mChqTmIwTo4ngTBIds0xiZt4sZmacvMYsYyYvosYY406MYRz3qCCKSCMCAqLIJoth\nk72BprnfP+o0abC7q4Cuql5+n+s6V1WdOsv92C13n+c5534UEZiZmTWkTb4DMDOzps/JwszM0nKy\nMDOztJwszMwsLScLMzNLy8nCzMzScrIwM7O0nCzMzCwtJwszM0urMN8BNJbDDz88BgwYkO8wzMya\nlZkzZ66LiJJ027WYZDFgwAAqKiryHYaZWbMiaVkm27kbyszM0nKyMDOztJwszMwsLScLMzNLy8nC\nzMzSajF3Qx2sx2et5NZnFrJqYyV9uhXz7QuPZsyI0nyHZWbWpLTqZPH4rJXc9NhcKquqAVi5sZKb\nHpsL4IRhZlZLq+6GuvWZhXsTRY3KqmpufWZhniIyM2uaWnWyWLWx8oDWm5m1Vq06WfTpVlzn+u4d\n2xIROY7GzKzpatXJ4tsXHk1x24J91gnYsK2Kq+6ZwZJ12/ITmJlZE9Oqk8WYEaXcctlxlHYrRkBp\nt2Ju+8zxfP9Tw3hj2Ydc+PMp3PbsQip3Vac9lplZS6aW0t1SXl4ejVlIcM3mHfz7kwv405ur6Nu9\nmB9efCznD+vVaMc3M2sKJM2MiPJ027XqK4uG9OxSxC+vGMGD155GcdsCvvK7Cr782xm8v2F7vkMz\nM8s5J4s0Th90GE/ecDbf++RQpi1ez/k/m8wvn3+XHVXumjKz1sPJIgNtC9owduQg/vzNUZw/rBc/\nf/4dLvzFFF5cuCbfoZmZ5URWk4WkGyXNk/SWpAclFUn6raQlkt5MlhPq2be61jaTshlnpnp3Lea/\nv3Ai9335FAokrr5nBuPuq2Cln8swsxYuawPckkqBqcCwiKiU9AjwJHAO8ERETEyz/9aI6JTp+Rp7\ngDudnburuevlJdz+wrsAfP3jQ7j27DLaFfpizcyaj6YywF0IFEsqBDoAq7J8vpxpX1jAV88dzPPf\nGMWoo0q49ZmFXPTLKUx9d12+QzMza3RZSxYRsRL4KbAcWA1siohnk6//XdIcST+X1L6eQxRJqpD0\nmqQx2YrzUPXt3oE7/r6ce64+meo9wZV3T+erD7zB6k3umjKzliNryUJSd2A0MBDoA3SUdCVwEzAU\nOBnoAXynnkP0Ty6NvgD8QtKgOs4xNkkoFWvXrs1GMzJ27tE9eeafRvKNC47i+fl/4bzbJjNhyntU\nVe/Ja1xmZo0hm91Q5wNLImJtRFQBjwFnRMTqSNkJ3AOcUtfOEbEqeV0MvASMqGObCRFRHhHlJSUl\n2WpHxoraFnD9eUN47sZRnF52GP/x5Nt88pcvM+299fkOzczskGQzWSwHTpPUQZKA84AFknoDJOvG\nAG/tv6Ok7jXdU5IOB84E5mcx1kbV/7AO3P2lk7nri+VUVlXz+Ttf44aHZrFm8458h2ZmdlCyNvlR\nREyXNBF4A9gNzAImAE9JKiFVs+9N4DoASeXAdRHxFeAY4A5Je0gltP+MiGaTLGqcP6wXZw4+nP/3\n0iLGT17Mnxes4cYLjuKq04+ksMB3TZlZ8+HaUDmyZN02fjBpHlPeWcvQIzrz4zHDOXlAj3yHZWat\nXFO5ddYSAw/vyL1Xn8z4K09kc2UVnxk/jW8+Mpu1W3bmOzQzs7ScLHJIEhcN783z3xzFP5wziEmz\nV/Lx217id9OWUr2nZVzhmVnL5GSRBx3aFfKdi4by1A0jOa60K9//0zwu+dVU3lj+Yb5DMzOrk5NF\nHg3u2Yn7v3Iqv/rCCNZt3cllv36V7/5hDhu27cp3aGZm+3CyyDNJfOr4Pvz5m+cwdmQZE2eu4OO3\nvcQD05ezx11TZtZEOFk0EZ3aF/K9Tx7DkzeczdG9OvO9P87l0l+/wpwVG/MdmpmZk0VTc1Svzjw0\n9jR+8bkTWLlxB6P/+xX+5Y9z2bjdXVNmlj9OFk2QJMaMKOWFb43iS2cM4MHXl/Px2ybzyIz33TVl\nZnnhZNGEdSlqyw8uPpYnvn42Aw/vyD//YQ6Xj3+Veas25Ts0M2tlnCyagWF9uvDouNO59fLjWbZ+\nOxffPpUfTprHpsqqfIdmZq2Ek0Uz0aaN+Ex5P1745jn83alHcu+0pZx322Qee2MFLaVki5k1XU4W\nzUzXDm358ZjhTPrqWZR2L+Ybj8zmc3e8xtsfbM53aGbWgjlZNFPH9e3KH//hDG657DjeWbOFv/2v\nqfzbE/PZssNdU2bW+JwsmrE2bcTnT+nPi988h8+W9+PuV5Zw3m2TmTR7lbumzKxROVm0AN07tuOW\ny47jj/94Jr26FHH9g7P4u7ums2jNlnyHZmYthJNFC3JCv248/tUz+fGY4by1chMX/eJlbnlqAdt2\n7s53aGbWzDlZtDAFbcTfn3YkL37rHC4dUcodkxdz/s8m89Tc1e6aMrOD5pnyWriKpRv4P3+ax4LV\nmzl7yOHcPHo4s9/fyK3PLGTVxkr6dCvm2xcezZgRpfkO1czyINOZ8pwsWoHd1Xu477Vl/OzZd9i+\nazeS2F2rbEhx2wJuuew4JwyzVsjTqtpehQVtuPrMgfz5W6NoV1iwT6IAqKyq5panFribyszqVZjv\nACx3enYuYkdVdZ3f/WXzTo774bMM7tmJo3p1YkjPzgzp1YmjenWmd9ciJOU4WjNrSpwsWpk+3YpZ\nubHyI+u7Frdl9Al9ePcvW3nh7TU8UrFi73ed2hc6iZi1ck4Wrcy3Lzyamx6bS2WtK4zitgX86JJj\n9xmz2LBtF+/+ZQvvrNnKor9s4Z0GksiQnqnkMaRXJ4b06kwfJxGzFierA9ySbgS+AgQwF7gaGA+M\nAmrqbH8pIt6sY9+rgH9NPv5bRNzb0Lk8wJ25x2etPOi7oepKIu+u2cK6rX+dnMlJxKz5yPvdUJJK\nganAsIiolPQI8CRwDvBERExsYN8eQAVQTirRzAROiogP69vHySK/apLIu2u2ppLJX7by7pqtrNu6\nc+82HdsVMLhXZ45KksjgpDvLScQsfzJNFtnuhioEiiVVAR2AVRnudyHwXERsAJD0HHAR8GBWorRD\n1qNjO04tO4xTyw7bZ/2H23bxzn5J5MWFa3l05l+7s5xEzJq+rCWLiFgp6afAcqASeDYinpX0BeDf\nJX0f+DPw3YjYud/upcD7tT6vSNbtQ9JYYCxA//79s9AKO1TdG0gi767ZmkokSTJxEjFrurKWLCR1\nB0YDA4GNwKOSrgRuAj4A2gETgO8AN++/ex2H/Eh/WURMSI5BeXm5HxJoRrp3bMcpA3twysAe+6w/\n0CRSMx7SUBI5lDEaM0vJZjfU+cCSiFgLIOkx4IyI+H3y/U5J9wDfqmPfFaTGNmr0BV7KXqjWVGSS\nRBYlr5kkkaXrtvF/n36byqo9AKzcWMlNj80FcMIwOwDZTBbLgdMkdSDVDXUeUCGpd0SsVupPwDHA\nW3Xs+wzwH8nVCcDfkLoisVbqQJLIS+/sm0T2V1lVzU+eftvJwuwAZHPMYrqkicAbwG5gFqkuo6ck\nlZDqanoTuA5AUjlwXUR8JSI2SPoxMCM53M01g91mtaVLIp+9Y1qd+63etIOzfvICw3p3YVifLntf\nS7sVezzErA4uJGgt2pn/+UKdT6x3KSpk5FElzF+9mSXrtlHzv0GXosIkeXTdm0QG9+xEu0KXUbOW\nqancOmuWV/U9sX7z6OF7u6G279rN2x9sYf6qzcxfvZn5qzbzwOvL2JGMc7QtEEN6dt7nCuSY3l3o\nWtw2L20yywcnC2vRahJCQ3dDdWhXyIn9u3Ni/+5711XvCZas28b81ZtZkCSQye+sZWKtsZDSbsX7\nJJBhvbvQt7u7saxlStsNJWl4RNQ1CN2kuBvKcmHNlh0sWF37KmQTi2t1Y3UuKvzIOMiQnp3djWVN\nVmN2Q42X1A74LfBARGw81ODMmquenYvo2bmIUUeV7F23fdduFn6wZW8X1vzVm3no9ff3dn21LRCD\ne3beN4n07kLXDu7GsuYjbbKIiLMkDQGuIXXr6+vAPRHxXNajM2sGOrQrZET/7ozYrxtr6fpt+4yD\nTHl3LX9446PdWMckyePYPu7GsqYr47uhJBWQei7iv4DNpG59/V5EPJa98DLnbihrDmp3Yy1YnUok\ni9duZU+tbqya5FFzFTKkVyfaFxbkN3BrsRqtG0rS8aRKi/8t8BxwcUS8IakPMA1oEsnCrDmoqxur\nclc1C/9SMw6yifmrNvPwjL92YxW2EYN7dvrIYHq3Du3y1QxrhTIZs/gVcCepq4i9N6xHxCpJ/1r/\nbmaWieJ2BZzQrxsn9Ou2d131nmDZ+m37jINMfXcdj72xcu82pd2KU1chfT7ajeV6WNbYMrkbqhNQ\nGRHVyec2QFFEbM9BfBlzN5S1Bmu37NzbfVWTRPbpxmpfSEnn9izbsJ3qPX/9f7u4bRtuuex4Jwz7\niEab/EjSa8D5EbE1+dyJVLnxMxol0kbiZGGt1f7dWI/MWMGu6j0f2a6wjRh1VAl9uxfTt3uHfV67\ndWjrgfVWqjFvnS2qSRQAEbE1KQ5oZk3A/t1Y97+2vM7tdu8JVm3awetLNrBl5+59vuvQrmC/JLJv\nQunuZNLqZZIstkk6MSLeAJB0EqkqsmbWBPXpVlxnPazSbsU8dcPZAGyqrGLFh9tZ8WFlsvz1/Qwn\nE6tDJsnin0hNXFQzJWpv4HPZC8nMDkV99bC+feHRez93LW5L1+KuHNuna53HcDKx/WXyUN4MSUOB\no0k9W/F2RFRlPTIzOyiZ1MNK52CSycrkfcXSDWze4WTS0mT0UJ6k4cAwoKhmXUT8LotxHTAPcJs1\nHZsqq5LkUdfVyfasJBPfLnxwGvOhvB+QmuJ0GPAk8AlgKtCkkoWZNR2pK5O2DOvTpc7vG0omB3Nl\nMnnhGr73x7f2dr15+tzGl8mYxeXAx4BZEXG1pF7AXdkNy8xassZOJgL27yOprKrm1mcWOlk0kkyS\nRWVE7JG0W1IXYA1QluW4zKwVO9BkcvMT8+vcblUdd4XZwckkWVRI6kaq5MdMYCvwelajMjNrwP7J\n5O6pS+q8XbhPt+Jch9ZiNTgji1IjSrdExMaIGA9cAFwVEVfnJDozswx8+8KjKW67b2XeosI2+9wu\nbIemwSuLiAhJjwMnJZ+X5iIoM7MDsf/twgGcMegwj1c0oky6oV6TdHJEzMh6NGZmB2nMiNK9yeHr\nD87ixbfXsHlHFV2KPCNhY8hkYuBzgWmS3pM0R9JcSXMyObikGyXNk/SWpAclFdX67nZJW+vZb4Ck\nSklvJsv4zJpjZgbjRpaxdeduHphed50sO3CZXFl84mAOLKkUuB4YFhGVkh4BrgB+K6kc6NbgAeC9\niDjhYM5tZq3b8NKunDX4cH4zdQlXnznAMw02gkyuLKKeJROFQLGkQqADsCqZnvVW4J8PPFwzs8yM\nG1XGmi07eXzWyvQbW1qZJIv/BZ5IXv8MLAaeSrdTRKwEfgosB1YDmyLiWeBrwKSIWJ3mEAMlzZI0\nWdLZGcRpZrbXWYMP59g+XbhjymL27Mn071urT9pkERHHRcTxyesQ4BRS5T4aJKk7MBoYCPQBOkr6\nIvAZ4PY0u68G+kfECOAbwAPJA4H7n2OspApJFWvXrk0Xkpm1IpIYN2oQi9du4/kFf8l3OM1eJlcW\n+0jmtTg5g03PB5ZExNqkSu1jwI+AwcAiSUuBDpIW1XGOnRGxPnk/E3gPOKqO7SZERHlElJeUlBxo\nU8yshfvk8CPo272Y8ZPfI5OiqVa/TAoJfqPWxzbAiUAmf8YvB05LZtWrBM4DfhYRe68qJG2NiMF1\nnLME2BAR1ZLKgCGkur/MzDJWWNCGa88u4weT5lGx7ENOHtAj3yE1W5lcWXSutbQnNXYxOt1OETEd\nmAi8AcxNzjWhvu0lXSLp5uTjSGCOpNnJMa6LiA0ZxGpmto/Plveje4e23DH5vXyH0qxlNJ9Fc+D5\nLMysPr94/h1+8fy7PHfjSIb06pzvcJqUTOezSHtlIem5pJBgzefukp451ADNzHLli6cPoKhtG+6Y\n4t7sg5VJN1RJRGys+RARHwI9sxeSmVnj6tGxHZ8r78ef3lzJ6k0uW34wMkkW1ZL613yQdCSZP5Rn\nZtYkfOXsMvYE3PPK0nyH0ixlkiz+BZgq6T5J9wFTgJuyG5aZWePq16MDf3tcbx6YvpxNlVX5DqfZ\nyeShvKdJ3S77MPAIcFJEeMzCzJqdsUmBwfunL8t3KM1OJgPclwJVEfFERPwPsFvSmOyHZmbWuIaX\nduXsIYdzzytL2VFVne9wmpVMuqF+EBGbaj4kg90/yF5IZmbZc92oQax1gcEDlkmyqGubTEqbm5k1\nOWcMOozhpV2Y4AKDBySTZFEh6WeSBkkqk/RzYGa2AzMzywZJjBs5iMXrtvHsfBcYzFQmyeLrwC5S\nA9yPAjuAr2YzKDOzbPrE8CPo18MFBg9EJndDbYuI7ybVXU+KiJsiYlsugjMzy4bCgjaMPbuMN9/f\nyIylH+Y7nGYhk7uhSiTdKulJSS/ULLkIzswsWy4/qR89OrZzgcEMZdINdT/wNqlJjH4ELAVmZDEm\nM7OsK25XwFWnD+DPb69h4Qdb8h1Ok5dJsjgsIu4m9azF5Ii4Bjgty3GZmWXdF08/kuK2BUxwgcG0\nMkkWNc/Fr5b0t5JGAH2zGJOZWU5079iOz53sAoOZyCRZ/JukrsA3gW8BdwE3ZjUqM7Mc+fJZAwng\nN1OX5DuUJi2Tu6GeiIhNEfFWRJyb3BE1KRfBmZllW78eHfjU8UmBwe0uMFifTK4szMxatLEjy9i2\nq5rfu8BgvZwszKzVO7ZPV0YeVeICgw1wsjAzA64bWca6rTv5owsM1iltQUBJ7YFPAwNqbx8RN2cv\nLDOz3Dp90GEcV9qVCVMW89nyfhS0Ub5DalIyubL4EzAa2A1sq7WYmbUYkhg3qowl67bx3PwP8h1O\nk5NJqfG+EXFR1iMxM8uzTwzvTf8eC/l/kxdz4bFHIPnqokYmVxavSjruYA4u6UZJ8yS9JelBSUW1\nvrtd0tYG9r1J0iJJCyVdeDDnNzM7EAVtxLUjy5j9/kZeX7Ih3+E0KZkki7OAmck/2nMkzZU0J91O\nkkqB64HyiBgOFABXJN+VA90a2HdYsu2xwEXAryUVZBCrmdkh+cxJfTmsYzvGu8DgPjLphvrEIR6/\nWFIV0AFYlfyjfyvwBeDSevYbDTwUETuBJZIWAacA0w4hFjOztIraFnDVGQP42XPv8PYHmxl6RJd8\nh9QkZPIE9zJSVwEXJ0u3ZF26/VYCPwWWA6uBTRHxLPA1YFJErG5g91Lg/VqfVyTrzMyyzgUGPyqT\n+SxuIFWmvGey/F7S1zPYrzupK4SBQB+go6QvAp8Bbk+3ex3rPjKdlaSxkiokVaxduzZdSGZmGenW\noR1XnNKPSW+uYtVGFxiEzMYsvgycGhHfj4jvkypPfm0G+50PLImItRFRBTxGaj6MwcAiSUuBDkkX\n0/5WAP1qfe4LrNp/o4iYkMzgV15SUpJBSGZmmakpMHi3CwwCmSULAbWff6+m7r/897ccOE1SB6Xu\nPzsP+FlEHBERAyJiALA9IgbXse8k4ApJ7SUNBIYAr2dwTjOzRtG3ewcuPr43D77uAoOQWbK4B5gu\n6YeSfgi8BtydbqeImA5MBN4A5ibnmlDf9pIukXRzsu884BFgPvA08NWIcMEWM8upcaMGsd0FBgFQ\nxEeGAj66kXQiqVtoBUyJiFnZDuxAlZeXR0VFRb7DMLMW5qrfvM68VZuY+p2PU9S25d3BL2lmRJSn\n267eKwtJXZLXHqTm3f49cB+wLFlnZtbijRtVxrqtu/jDGyvyHUpeNdQN9UDyOhOoqLXUfDYza/FO\nLzuM4/t25c4pi6nek74npqWqN1lExKeS14ERUVZrGRgRZbkL0cwsfyRx3ahBLF2/nWfntd4Cg5k8\nZ/HnTNaZmbVUFx57BEce1oHxk98jk3HelqihMYuiZGzicEndJfVIlgGkHrIzM2sVCtqIa88uY/aK\nTby2uHUWGGzoymIcqfGJoclrzfIn4L+zH5qZWdNx+Ul9ObxTO+6Y0joLDDY0ZvHLiBgIfKvWWMXA\niPhYRPwqhzGameVdUdsCvnTGAF5auJYFqzfnO5ycy6SQ4O2Shkv6rKQv1iy5CM7MrCm58rQj6dCu\ngDtbYYHBTAa4f0Cq8N/twLnA/wUuyXJcZmZNTrcO7bji5P5Mmr2Kla2swGAm5T4uJ1XX6YOIuBr4\nGNA+q1GZmTVRXz57IAB3v9y6CgxmkiwqI2IPsDt5qnsN4OcszKxVKu1WzCUf68NDM5azcfuufIeT\nM5kkiwpJ3YA7Sd0N9QauAGtmrdjYUWWpAoOvtZ4Cg5kMcP9jRGyMiPHABcBVSXeUmVmrNPSILpxz\ndAn3vLKUHVWtoyB2Qw/lnbj/AvQACpP3Zmat1riRg1i/bRcTZ7aOAoOFDXx3W/JaBJQDs0mVKD8e\nmE6qZLmZWat0WlkPPtavG3e+vJjPn9KfgjaZzAnXfDX0UN65EXEusAw4MZm+9CRgBFDXVKhmZq2G\nJK4bWcay9dt5phUUGMxkgHtoRMyt+RARbwEnZC8kM7Pm4W+OPYIBraTAYCbJYoGkuySdI2mUpDuB\nBdkOzMysqStoI64dWcacFZuYtnh9vsPJqkySxdXAPOAG4J9IzYvtu6HMzIBPn5gUGJzcskuAZHLr\n7I6I+HlEXJosP4+IHbkIzsysqStqW8DVZw5k8jstu8BgQ7fOPpK8zpU0Z/8ldyGamTVtV56aKjB4\nx+SWW768oVtnb0heP5WLQMzMmquuHdry+VP689tXl/KtC4+mb/cO+Q6p0TV06+zq5HVZXUvuQjQz\na/q+fNZABNw9tWUWGGyoG2qLpM11LFskZdQxJ+lGSfMkvSXpwWSq1rslzU66syZK6lTHfgMkVUp6\nM1nGH0ojzcyyrU+3Yi45oQ8Pvf4+H25reQUGG7qy6BwRXepYOkdEl3QHllQKXA+UR8RwoAC4Argx\nmW3veGA58LV6DvFeRJyQLNcdeNPMzHJr7MgyKququa8FFhjM5NZZACT1lNS/Zslwt0KgWFIh0AFY\nFRGbk+MJKAZa9pMsZtZqDD2iC+ceXcJvX215BQYzmSnvEknvAkuAycBS4Kl0+0XESuCnpK4eVgOb\nIuLZ5Jj3AB8AQ0nNwFeXgZJmSZos6ex6YhsrqUJSxdq1a9OFZGaWddeNGsSGbbt4tIUVGMzkyuLH\nwGnAOxExkNSsea+k20lSd2A0MBDoA3SUdCVAUuK8D6knwT9Xx+6rgf4RMQL4BvBAMvHSPiJiQlKz\nqrykpCSDppiZZdcpA3twQr9u3DllMbur9+Q7nEaTSbKoioj1QBtJbSLiRTKrDXU+sCQi1kZEFfAY\ncEbNlxFRDTwMfHr/HSNiZ3JOImIm8B5wVAbnNDPLK0lcN6qM5Ru283QLKjCYSbLYmNyxNAW4X9Iv\ngd0Z7LccOE1Sh2R84jxSdaYGw94xi4uBt/ffUVKJpILkfRkwBGjZz9KbWYtxwbAjGHh4R+6YvLjF\nFBjMJFmMBiqBG4GnSf2Vf3G6nSJiOjCR1DSsc5NzTQDulTQ3WdcbuBn2jo3cnOw+EpgjaXZyjOsi\nYsMBtMvMLG8K2oixI8uYu3IT095rGQUGVV/Wk/Qr4IGIeDW3IR2c8vLyqKioyHcYZmYA7Kiq5qyf\nvMgxvTtz35dPzXc49ZI0MyLK023X0JXFu8BtkpZK+okkz2FhZpahVIHBAbz87jrmrdqU73AOWUMP\n5f0yIk4HRgEbgHskLZD0fUkebDYzS+PK046kY7sCJkxp/kOumZQoXxYRP0luY/0CcCme/MjMLK2u\nxW35wqn9eWLOat7fsD3f4RySTB7KayvpYkn3k3oY7x3quN3VzMw+6poWUmCwoUKCF0j6DbACGAs8\nCQyKiM9FxOO5CtDMrDnr3bWY0SeU8tCM5WxoxgUGG7qy+B4wDTgmIi6OiPsjYluO4jIzazHGjSpj\nR9Ue7pvWfAsMNjTAfW5E3OnnG8zMDs1RvTpz3tCe3DttKZW7mmeBwYyrzpqZ2cEbt7fA4Pv5DuWg\nOFmYmeXAyQO6M6J/N+58uXkWGHSyMDPLgVSBwUG8v6GSp95qfgUGnSzMzHLkgmN6UXZ4R+6Y8l6z\nKzDoZGFmliNtkgKDb63czCuLmleBQScLM7McGjOilJLO7bljynv5DuWAOFmYmeVQUdsCrjlzIC+/\nu463VjafAoNOFmZmOfaFU/vTqX1hsyow6GRhZpZjfy0wuKrZFBh0sjAzy4NrzhxIQRtx18vN4+rC\nycLMLA+O6FrEmBNKebji/WZRYNDJwswsT8aOTBUY/N20pfkOJS0nCzOzPBnSqzPnH9OTe19dyvZd\nu/MdToOcLMzM8mjcqEF8uL2KRytW5DuUBjlZmJnl0ckDenDSkd2bfIHBrCYLSTdKmifpLUkPSiqS\ndLek2ZLmSJooqVM9+94kaZGkhZIuzGacZmb5NG5kGSs+rOTJJlxgMGvJQlIpcD1QHhHDgQLgCuDG\niPhYRBwPLAe+Vse+w5JtjwUuAn4tqSBbsZqZ5dP5x/SirKQj419qugUGs90NVQgUSyoEOgCrImIz\ngCQBxUBd/2VGAw9FxM6IWAIsAk7JcqxmZnnRpo0YN7KM+as3M3XRunyHU6esJYuIWAn8lNTVw2pg\nU0Q8CyDpHuADYChwex27lwK1p5NakawzM2uRxowopWfn9twxuWk+pJfNbqjupK4QBgJ9gI6SrgSI\niKuTdQuAz9W1ex3rPnIFImmspApJFWvXrm202M3Mcq19YQHXnDWQqYuaZoHBbHZDnQ8siYi1EVEF\nPAacUfNlRFQDDwOfrmPfFUC/Wp/7Aqv23ygiJkREeUSUl5SUNGrwZma5VlNgcPzkple+PJvJYjlw\nmqQOyfjEecACSYNh75jFxcDQD2U6AAAKTklEQVTbdew7CbhCUntJA4EhwOtZjNXMLO+6FLXl707t\nz5NzV7N8fdMqMJjNMYvpwETgDWBucq4JwL2S5ibregM3A0i6RNLNyb7zgEeA+cDTwFeTKxEzsxbt\nmrOSAoNTm9bYhZrqbVoHqry8PCoqKvIdhpnZIfvnibOZNHsVr3zn4xzWqX1WzyVpZkSUp9vOT3Cb\nmTUxNQUG7522LN+h7OVkYWbWxAzu2Znzj+nF76Y1nQKDThZmZk3QP5xTxsbtVTwy4/30G+eAk4WZ\nWRN00pE9KD+yO3e+vKRJFBh0sjAza6LGjRrEyo2V/O/c1fkOxcnCzKypOm9oTwb37MT4yYvzXmDQ\nycLMrIlq00aMHVnGgtWbefnd/BYYdLIwM2vCRp/Qh15d2nPHlPyWAHGyMDNrwtoXFnDNmQN5ZdF6\n5q7IX4FBJwszsybu86f2p3P7Qsbn8erCycLMrInrUtSWvzvtSJ6au5pl67flJQYnCzOzZuDqMwdQ\n2KYNd728JC/nd7IwM2sGenUp4tIRpTxS8T7rtu7M+fmdLMzMmolrR5axq3oPv3t1ac7P7WRhZtZM\nDO7ZiQuO6cW905axbWduCww6WZiZNSPjRg1iU2UVj1TktsCgk4WZWTNy0pHdOXlAd+56eQlVOSww\n6GRhZtbMjBuZFBick7sCg04WZmbNzMeH9mRIz06Mn/xezgoMOlmYmTUzNQUG3/5gC1NyVGDQycLM\nrBkafUIpXYoKuPZ3FQz87v9y5n++wOOzVmbtfIVZO7KZmWXNk3NXs33XHnbvSXVDrdxYyU2PzQVg\nzIjSRj+fryzMzJqhW59ZuDdR1KisqubWZxZm5XxZTRaSbpQ0T9Jbkh6UVCTpfkkLk3W/kdS2nn2r\nJb2ZLJOyGaeZWXOzamPlAa0/VFlLFpJKgeuB8ogYDhQAVwD3A0OB44Bi4Cv1HKIyIk5IlkuyFaeZ\nWXPUp1vxAa0/VNnuhioEiiUVAh2AVRHxZCSA14G+WY7BzKzF+faFR1PctmCfdcVtC/j2hUdn5XxZ\nSxYRsRL4KbAcWA1siohna75Pup/+Hni6nkMUSaqQ9JqkMdmK08ysORozopRbLjuO0m7FCCjtVswt\nlx2XlcFtyOLdUJK6A6OBgcBG4FFJV0bE75NNfg1MiYiX6zlE/4hYJakMeEHS3IjYZ5ooSWOBsQD9\n+/fPSjvMzJqqMSNKs5Yc9pfNbqjzgSURsTYiqoDHgDMAJP0AKAG+Ud/OEbEqeV0MvASMqGObCRFR\nHhHlJSUljd8CMzMDspsslgOnSeogScB5wAJJXwEuBD4fEXVWwZLUXVL75P3hwJnA/CzGamZmDcjm\nmMV0YCLwBjA3OdcEYDzQC5iW3Bb7fQBJ5ZLuSnY/BqiQNBt4EfjPiHCyMDPLE+WqCFW2lZeXR0VF\nRb7DMDNrViTNjIjydNv5CW4zM0urxVxZSFoLLDuEQxwO5KZ8Y9PR2trc2toLbnNrcShtPjIi0t4h\n1GKSxaGSVJHJpVhL0tra3NraC25za5GLNrsbyszM0nKyMDOztJws/mpCvgPIg9bW5tbWXnCbW4us\nt9ljFmZmlpavLMzMLK0WnywkXZRMtrRI0nfr+L69pIeT76dLGpCsP0zSi5K2SvpVruNuLBm0f6Sk\nNyTtlnR5PmJsDIfwc75A0kxJc5PXj+c69oN1CG0eIKmy1uRi43Mde2NoLb/btWXQ5i9JWlvrZ1vf\nfEEHLiJa7EJqwqX3gDKgHTAbGLbfNv8IjE/eXwE8nLzvCJwFXAf8Kt9tyWL7BwDHA78DLs93zHn4\nOY8A+iTvhwMr892eHLR5APBWvtuQg/Y3+9/tg2jzl7L171VLv7I4BVgUEYsjYhfwEKmy6bWNBu5N\n3k8EzpOkiNgWEVOBHbkLt9GlbX9ELI2IOUCdRR2biUP5Oc+KpMIxMI/UPCrtcxL1oTnoNucwxmxq\nLb/btWXyM8+alp4sSoH3a31ekayrc5uI2A1sAg7LSXTZl0n7W4LG+jl/GpgVETuzFGdjOtQ2D5Q0\nS9JkSWdnO9gsaC2/27Vl2uZPS5ojaaKkfo118paeLOr6K2r/278y2aa5asltq+2Qf86SjgV+Aoxr\nxLiy6VDavJrU5GIjSM0p84CkLo0cX7a1lt/t2jJp8/8AAyLieOB5/nplechaerJYAdTOrH2BVfVt\nk8wV3hXYkJPosi+T9rcEh/RzltQX+CPwxdhvNsYm7KDbHBE7I2I9QETMJNUPflTWI25creV3u7a0\nbY6I9bWujO8ETmqsk7f0ZDEDGCJpoKR2pAb5Ju23zSTgquT95cALkYwUtQCZtL8lOOifs6RuwP8C\nN0XEKzmL+NAdSptLJBUAJNMWDwEW5yjuxtJafrdrS9tmSb1rfbwEWNBoZ8/3CH8O7iD4JPAOqb+e\n/iVZdzNwSfK+CHgUWAS8DpTV2ncpqb8+t5LK6sNyHX8O2n9y0rZtwHpgXr5jzuXPGfjXpO1v1lp6\n5rs9WW7zp0kN5s8mNTnZxfluS5ba3yJ+tw+wzbfU+tm+CAxtrHP7CW4zM0urpXdDmZlZI3CyMDOz\ntJwszMwsLScLMzNLy8nCzMzScrKwVkPS1hyc45K6qoFm+ZznSDojl+e01qcw3wGYNTeSCiKiuq7v\nImISWXg4TFJhpOo71eUcUs8CvdrY5zWr4SsLa5UkfVvSjKTg2o9qrX88mddinqSxtdZvlXSzpOnA\n6ZKWSvpRMl/CXElDk+2+VDP/iaTfSvovSa9KWlwzp4KkNpJ+nZzjCUlP1jXfgqSXJP2HpMnADZIu\nTualmCXpeUm9kjkqrgNuTOYvODt5QvsPSftmSDozm/8trXXwlYW1OpL+hlSJi1NIFWebJGlkREwB\nromIDZKKgRmS/hCpOkodSc0B8f3kGADrIuJESf8IfAuoa6KZ3qTmRRlK6opjInAZqbkWjgN6kirJ\n8Jt6wu0WEaOSc3YHTouISCa1+eeI+GYyedHWiPhpst0DwM8jYqqk/sAzwDEH/R/MDCcLa53+Jllm\nJZ87kUoeU4DrJV2arO+XrF8PVAN/2O84jyWvM0klgLo8HhF7gPmSeiXrzgIeTdZ/IOnFBmJ9uNb7\nvsDDSf2fdsCSevY5HxhWa+qKLpI6R8SWBs5j1iAnC2uNBNwSEXfss1I6h9Q/tKdHxHZJL5GqrwSw\no45xiprqntXU//9S7bkxtN9rJrbVen878LOImJTE+sN69mlDqg2VB3AeswZ5zMJao2eAayR1ApBU\nKqknqRLeHyaJYihwWpbOP5XUBDVtkquNczLcryuwMnl/Va31W4DOtT4/C3yt5oOkEw4+VLMUJwtr\ndSLiWeABYJqkuaTGEToDTwOFkuYAPwZey1IIfyBVDfUt4A5gOqlZ7NL5IfCopJeBdbXW/w9wac0A\nN3A9UJ4M3s8nNQBudkhcddYsDyR1ioitkg4jVT78zIj4IN9xmdXHYxZm+fFEMvFSO+DHThTW1PnK\nwszM0vKYhZmZpeVkYWZmaTlZmJlZWk4WZmaWlpOFmZml5WRhZmZp/X89ePAISMHI9wAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bf6c8b128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = ['0.01','0.1','0.02','0.05','0.1','0.5']\n",
    "scores = [85.68,85.1,84.94,84.76,84.64,82.56]\n",
    "plt.plot(scores,'-o')\n",
    "plt.xticks(range(len(lr)),lr)\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('Validation accuracy')\n",
    "#plt.savefig('learning_rate.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c2224a780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "voc_size = ['5k','10k','20k','50k','100k','200k']\n",
    "scores = [82.66,84.06,85.1,87.06,87.32,87.44]\n",
    "plt.plot(scores,'-o')\n",
    "plt.xticks(range(len(scores)),voc_size)\n",
    "plt.xlabel('vocabulary size')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.savefig('vocabulary_size.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA, pred, label = test_model_2(val_loader,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
