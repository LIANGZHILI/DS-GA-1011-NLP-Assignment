{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS-GA 1011 Fall 2018 Assignment 2\n",
    "#### RNN/CNN-based Natural Language Inference\n",
    "Liangzhi Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(2018)\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amberli/hw2/hi\n"
     ]
    }
   ],
   "source": [
    "cd /home/amberli/hw2/hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available and torch.has_cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available and torch.has_cudnn:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def readtsvfile(filename):\n",
    "    texts = []\n",
    "    with open(filename) as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            texts.append(row)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 100000\n",
      "validation data: 1000\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens = readtsvfile('snli_train.tsv')\n",
    "val_data_tokens = readtsvfile('snli_val.tsv')\n",
    "train_data_tokens = train_data_tokens[1::]\n",
    "val_data_tokens = val_data_tokens[1::]\n",
    "print('train data: {}'.format(len(train_data_tokens)))\n",
    "print('validation data: {}'.format(len(val_data_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pretrained word embeddings from fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ft_home = './'\n",
    "words_to_load = 50000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load+2, 300))\n",
    "    loaded_embeddings_ft[1,:]=np.random.randn(300)*0.01\n",
    "    words_ft = {'<pad>':0,'<unk>':1}\n",
    "    idx2words_ft = {0:'<pad>',1:'<unk>'}\n",
    "    ordered_words_ft = ['<pad>','<unk>']\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+2\n",
    "        idx2words_ft[i+2] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize sentence 1 and sentence 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    for i in range(len(data)):\n",
    "        data[i][0]=data[i][0].split()\n",
    "        data[i][1]=data[i][1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize(train_data_tokens)\n",
    "tokenize(val_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_dic = {'contradiction':1,'entailment':0,'neutral':2}\n",
    "def token2id(data):\n",
    "    data_id = []\n",
    "    for i in range(len(data)):\n",
    "        sent_id_1 = []\n",
    "        for word in data[i][0]:\n",
    "            if word in words_ft:\n",
    "                sent_id_1.append(words_ft[word])\n",
    "            else:\n",
    "                word = '<unk>'\n",
    "                sent_id_1.append(words_ft[word])\n",
    "        sent_id_2 = []\n",
    "        for word in data[i][1]:\n",
    "            if word in words_ft:\n",
    "                sent_id_2.append(words_ft[word])\n",
    "            else:\n",
    "                word = '<unk>'\n",
    "                sent_id_2.append(words_ft[word])\n",
    "        data[i][2] = label_dic[data[i][2]]        \n",
    "        data_id.append([sent_id_1,sent_id_2,data[i][2]])\n",
    "    return data_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_id = token2id(train_data_tokens)\n",
    "val_id = token2id(val_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "len1 = []\n",
    "len2 = []\n",
    "for i in range(1000):\n",
    "    len1.append(len(train_data_tokens[i][0]))\n",
    "    len2.append(len(train_data_tokens[i][1]))\n",
    "print(max(len1))\n",
    "print(max(len2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## transform to dataloader\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 40\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_tuple):\n",
    "        \"\"\"\n",
    "        @param data_list: list of my tokens \n",
    "        @param target_list: list of my targets \n",
    "\n",
    "        \"\"\"\n",
    "        #data_list1, data_list2, target_list\n",
    "        self.data_list1, self.data_list2, self.target_list = zip(*data_tuple)\n",
    "        assert (len(self.data_list1) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx1 = self.data_list1[key][:MAX_SENTENCE_LENGTH]\n",
    "        token_idx2 = self.data_list2[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx1, token_idx2, len(token_idx1),len(token_idx2), label]\n",
    "\n",
    "def my_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    label_list = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list1.append(datum[2])\n",
    "        length_list2.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        padded_vec2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "    #ind_dec_order_1 = np.argsort(length_list1)[::-1]\n",
    "    #ind_dec_order_2 = np.argsort(length_list2)[::-1]\n",
    "    #data_list1 = np.array(data_list1)[ind_dec_order_1]\n",
    "    #length_list1 = np.array(length_list1)[ind_dec_order_1]\n",
    "\n",
    "    #data_list2 = np.array(data_list2)[ind_dec_order_2]\n",
    "    #length_list2 = np.array(length_list2)[ind_dec_order_2]\n",
    "    #label_list = np.array(label_list)[ind_dec_order]\n",
    "    \n",
    "    # keep track of original order:\n",
    "    #ind_order_1 = np.argsort(ind_dec_order_1)\n",
    "    #ind_order_2 = np.argsort(ind_dec_order_2)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(data_list1)).to(device),torch.from_numpy(np.array(data_list2)).to(device), \n",
    "            torch.LongTensor(length_list1).to(device), \n",
    "            torch.LongTensor(length_list2).to(device), torch.LongTensor(label_list).to(device)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = myDataset(train_data_indices, train_targets)\n",
    "#val_loader = myDataset(val_data_indices, val_targets)\n",
    "#test_loader = myDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = myDataset(train_id)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=my_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = myDataset(val_id)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=my_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size,kernel_size):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft))  #nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, padding=1)\n",
    "        #self.maxpool = nn.MaxPool1d(kernel_size = 3, padding=1)\n",
    "        self.maxpool = nn.MaxPool1d(MAX_SENTENCE_LENGTH-kernel_size+1)\n",
    "        self.linear1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x1, x2, lengths1, lengths2):\n",
    "        batch_size, seq_len = x1.size()\n",
    "\n",
    "        embed_1 = self.embedding(x1).float()\n",
    "        embed_2 = self.embedding(x2).float()\n",
    "        \n",
    "        hidden_1 = self.conv1(embed_1.transpose(1,2)).transpose(1,2)\n",
    "        #hidden_1 = F.relu(hidden_1)\n",
    "        hidden_1 = F.relu(hidden_1.contiguous().view(-1, hidden_1.size(-1))).view(batch_size, seq_len, hidden_1.size(-1))\n",
    "        #hidden_1 = self.maxpool(hidden_1).view(batch_size,hidden_1.size(-1))\n",
    "        \n",
    "        hidden_1 = self.conv2(hidden_1.transpose(1,2)).transpose(1,2)\n",
    "        #hidden_1 = F.relu(hidden_1)\n",
    "        hidden_1 = F.relu(hidden_1.contiguous().view(-1, hidden_1.size(-1))).view(batch_size, seq_len, hidden_1.size(-1))\n",
    "        #hidden_1 = self.maxpool(hidden_1).view(batch_size,hidden_1.size(-1))\n",
    "        hidden_1 = torch.max(hidden_1,dim=1)[0]\n",
    "\n",
    "        #hidden_1 = torch.sum(hidden_1, dim=1)\n",
    "\n",
    "        hidden_2 = self.conv1(embed_2.transpose(1,2)).transpose(1,2)\n",
    "        #hidden_2 = F.relu(hidden_2)\n",
    "        hidden_2 = F.relu(hidden_2.contiguous().view(-1, hidden_2.size(-1))).view(batch_size, seq_len, hidden_2.size(-1))\n",
    "        #hidden_2 = self.maxpool(hidden_2).view(batch_size,hidden_2.size(-1))\n",
    "        hidden_2 = self.conv2(hidden_2.transpose(1,2)).transpose(1,2)\n",
    "        #hidden_2 = F.relu(hidden_2)\n",
    "        hidden_2 = F.relu(hidden_2.contiguous().view(-1, hidden_2.size(-1))).view(batch_size, seq_len, hidden_2.size(-1))\n",
    "        #hidden_2 = self.maxpool(hidden_2).view(batch_size,hidden_2.size(-1))\n",
    "        hidden_2 = torch.max(hidden_2,dim=1)[0]\n",
    "        \n",
    "        #hidden_2 = torch.sum(hidden_2, dim=1)\n",
    "        \n",
    "        ## concat:\n",
    "        hidden = torch.cat((hidden_1, hidden_2), 1)\n",
    "        \n",
    "        ## element-wise multiplication\n",
    "        #hidden = hidden_1 * hidden_2\n",
    "\n",
    "\n",
    "        logits = self.linear1(hidden)\n",
    "        logits = torch.tanh(logits)\n",
    "        logits = self.linear2(logits)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:43: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 62.1, Train Acc: 62.079, loss: 0.29427653551101685\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 64.3, Train Acc: 64.389, loss: 0.5650834441184998\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 64.8, Train Acc: 65.679, loss: 0.8290256857872009\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 63.4, Train Acc: 67.147, loss: 0.25121408700942993\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 64.9, Train Acc: 68.271, loss: 0.5005056262016296\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 64.5, Train Acc: 69.121, loss: 0.7474497556686401\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 66.2, Train Acc: 70.414, loss: 0.23049212992191315\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 65.2, Train Acc: 71.131, loss: 0.46164774894714355\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 66.5, Train Acc: 72.998, loss: 0.6938256025314331\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 66.0, Train Acc: 73.766, loss: 0.2125099003314972\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 64.8, Train Acc: 74.949, loss: 0.42803335189819336\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 66.2, Train Acc: 74.829, loss: 0.6457338929176331\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 66.6, Train Acc: 76.786, loss: 0.19616800546646118\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 64.9, Train Acc: 77.053, loss: 0.396338552236557\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 67.5, Train Acc: 78.427, loss: 0.5993216037750244\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 65.9, Train Acc: 79.495, loss: 0.17910228669643402\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 66.4, Train Acc: 80.838, loss: 0.3629697859287262\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 66.3, Train Acc: 81.031, loss: 0.5522236824035645\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 66.1, Train Acc: 82.043, loss: 0.1603817194700241\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 66.1, Train Acc: 82.974, loss: 0.3280535340309143\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 66.7, Train Acc: 83.55, loss: 0.5033056139945984\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 67.5, Train Acc: 84.233, loss: 0.14398793876171112\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 69.2, Train Acc: 85.367, loss: 0.2963186502456665\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 66.8, Train Acc: 85.06, loss: 0.4562409222126007\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 66.6, Train Acc: 86.432, loss: 0.12683680653572083\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 67.5, Train Acc: 86.709, loss: 0.26561930775642395\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 67.5, Train Acc: 87.541, loss: 0.41092708706855774\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 65.9, Train Acc: 88.589, loss: 0.11218380928039551\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 65.1, Train Acc: 88.473, loss: 0.23544417321681976\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 67.2, Train Acc: 88.757, loss: 0.3676493763923645\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1,data2, lengths1,lengths2, labels in loader:\n",
    "        data_batch_1,data_batch_2, lengths_batch1,lengths_batch2, label_batch= data1,data2, lengths1, lengths2, labels\n",
    "        outputs = F.softmax(model(data_batch_1,data_batch_2, lengths_batch1,lengths_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = CNN(emb_size=300, hidden_size=200, num_layers=2, num_classes=3, vocab_size=50000,kernel_size=3)\n",
    "model.cuda()\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "Train_Acc = []\n",
    "Val_Acc = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for i, (data1, data2, lengths1, lengths2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data1,data2, lengths1, lengths2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.data[0]\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 1000 iterations\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            train_acc = test_model(train_loader, model)\n",
    "            Train_Acc.append(train_acc)\n",
    "            Val_Acc.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Train Acc: {}, loss: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc, train_acc,  train_loss / len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.4"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(Val_Acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:39: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 58.6\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 62.3\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 62.8\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 60.5\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 61.8\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 63.2\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 62.6\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 64.1\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 65.1\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 65.1\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 66.7\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 65.3\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 64.9\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 65.8\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 66.1\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 66.7\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 65.7\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 67.4\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 66.7\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 67.6\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 66.5\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 67.8\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 67.3\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 66.5\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 67.9\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 66.6\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 67.7\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 66.9\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 67.5\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 66.8\n"
     ]
    }
   ],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1,data2, lengths1,lengths2, labels in loader:\n",
    "        data_batch_1,data_batch_2, lengths_batch1,lengths_batch2, label_batch= data1,data2, lengths1, lengths2, labels\n",
    "        outputs = F.softmax(model(data_batch_1,data_batch_2, lengths_batch1,lengths_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = CNN(emb_size=300, hidden_size=50, num_layers=2, num_classes=3, vocab_size=50000,kernel_size=3)\n",
    "model.cuda()\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for i, (data1, data2, lengths1, lengths2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data1,data2, lengths1, lengths2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.data[0]\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 1000 iterations\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN *** bidirectional, gru, with 2 fully connected layers\n",
    "PAD_IDX = 0\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size # embed into continuous vector\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft))#.cuda()\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size)\n",
    "        hidden = hidden.cuda()\n",
    "\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, x1, x2, lengths1,lengths2):\n",
    "        # reset hidden state\n",
    "\n",
    "        batch_size, seq_len = x1.size()\n",
    "\n",
    "        hidden_1 = self.init_hidden(batch_size)\n",
    "        hidden_2 = self.init_hidden(batch_size)\n",
    "\n",
    "        ## sort x1, x2 in descending order\n",
    "        _, ind1 = torch.sort(lengths1, dim=0, descending=True)\n",
    "        _, ind2 = torch.sort(lengths2, dim=0, descending=True)\n",
    "        _, unind1 = torch.sort(ind1, dim=0)\n",
    "        _, unind2 = torch.sort(ind2, dim=0)\n",
    "        x1 = x1.index_select(0,ind1)\n",
    "        x2 = x2.index_select(0,ind2)\n",
    "        lengths1 = lengths1.index_select(0,ind1)\n",
    "        lengths2 = lengths2.index_select(0,ind2)\n",
    "        \n",
    "        #print(type(x1))\n",
    "        # get embedding of characters\n",
    "        embed_1 = self.embedding(x1).float()\n",
    "        \n",
    "        embed_2 = self.embedding(x2).float()\n",
    "\n",
    "        # pack padded sequence\n",
    "        embed_1 = torch.nn.utils.rnn.pack_padded_sequence(embed_1, lengths1.cpu().numpy(), batch_first=True)\n",
    "        embed_2 = torch.nn.utils.rnn.pack_padded_sequence(embed_2, lengths2.cpu().numpy(), batch_first=True)\n",
    "\n",
    "        _, hidden_1 = self.gru(embed_1, hidden_1)\n",
    "        _, hidden_2 = self.gru(embed_2, hidden_2)\n",
    "\n",
    "\n",
    "        hidden_1 = hidden_1.index_select(1,unind1)\n",
    "        #print(hidden_1.shape)\n",
    "        hidden_2 = hidden_2.index_select(1,unind2)\n",
    "        \n",
    "        \n",
    "        hidden_1 = torch.max(hidden_1, dim=0)[0]\n",
    "        #print(hidden_1.shape)\n",
    "        hidden_2 = torch.max(hidden_2, dim=0)[0]\n",
    "        \n",
    "        \n",
    "        ## concat:\n",
    "        #hidden = torch.cat((hidden_1, hidden_2), 1)\n",
    "        \n",
    "        ## element wise multiplication:\n",
    "        hidden = hidden_1 * hidden_2\n",
    "\n",
    "        logits = self.linear1(hidden)\n",
    "        logits = torch.tanh(logits)\n",
    "        logits = self.linear2(logits)        \n",
    "\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start:\n",
      "\n",
      "483.42864085\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 54.8, Train Acc: 55.716, loss: 0.8937720060348511\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 59.6, Train Acc: 59.627, loss: 0.7011268734931946\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 60.3, Train Acc: 63.164, loss: 0.7405701875686646\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 64.7, Train Acc: 66.009, loss: 0.7355993390083313\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 69.0, Train Acc: 68.21, loss: 0.7544618248939514\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 68.5, Train Acc: 68.758, loss: 0.5797196626663208\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 67.9, Train Acc: 70.058, loss: 0.8906170129776001\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 66.5, Train Acc: 70.626, loss: 0.8185067772865295\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 69.9, Train Acc: 71.966, loss: 0.5813447833061218\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 67.4, Train Acc: 72.873, loss: 0.6433400511741638\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 69.0, Train Acc: 73.84, loss: 0.7496302723884583\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 69.6, Train Acc: 73.609, loss: 0.5110811591148376\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 69.2, Train Acc: 75.07, loss: 0.6786374449729919\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 68.6, Train Acc: 75.66, loss: 0.91314697265625\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 69.7, Train Acc: 76.573, loss: 0.8864812850952148\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 71.0, Train Acc: 77.161, loss: 0.9604626297950745\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 70.3, Train Acc: 78.228, loss: 0.6087610125541687\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 70.5, Train Acc: 78.876, loss: 0.7133792638778687\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 70.1, Train Acc: 79.08, loss: 0.5084704160690308\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 71.5, Train Acc: 79.772, loss: 0.3023885488510132\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 71.5, Train Acc: 80.704, loss: 0.8659400343894958\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 71.6, Train Acc: 80.655, loss: 0.4992053806781769\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 71.5, Train Acc: 81.665, loss: 0.49229973554611206\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 71.7, Train Acc: 82.42, loss: 0.48181700706481934\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 70.6, Train Acc: 83.47, loss: 0.3259086608886719\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 71.6, Train Acc: 83.569, loss: 0.48146331310272217\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 71.1, Train Acc: 84.113, loss: 0.3792123794555664\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 70.8, Train Acc: 84.413, loss: 0.5344262719154358\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 71.6, Train Acc: 85.732, loss: 0.2781769037246704\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 70.8, Train Acc: 85.999, loss: 0.23842042684555054\n",
      "end\n",
      "542.8847917833333\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1,data2, lengths1,lengths2, labels in loader:\n",
    "        data_batch_1,data_batch_2, lengths_batch1,lengths_batch2, label_batch = data1,data2, lengths1, lengths2, labels \n",
    "        outputs = F.softmax(model(data_batch_1,data_batch_2, lengths_batch1,lengths_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model = RNN(emb_size=300, hidden_size=100, num_layers=2, num_classes=3, vocab_size=50000)\n",
    "model.cuda()\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "Train_Acc = []\n",
    "Val_Acc = []\n",
    "print('start:\\n')\n",
    "print(time.clock() / 60)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data1, data2, lengths1, lengths2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(data1,data2, lengths1, lengths2)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            train_acc = test_model(train_loader, model)\n",
    "            Train_Acc.append(train_acc)\n",
    "            Val_Acc.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Train Acc: {}, loss: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc, train_acc,  loss))\n",
    "\n",
    "print('end')\n",
    "print(time.clock() / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start:\n",
      "\n",
      "602.05972495\n",
      "Epoch: [1/10], Step: [1001/3125], Validation Acc: 55.5\n",
      "Epoch: [1/10], Step: [2001/3125], Validation Acc: 61.9\n",
      "Epoch: [1/10], Step: [3001/3125], Validation Acc: 66.1\n",
      "Epoch: [2/10], Step: [1001/3125], Validation Acc: 65.4\n",
      "Epoch: [2/10], Step: [2001/3125], Validation Acc: 68.4\n",
      "Epoch: [2/10], Step: [3001/3125], Validation Acc: 68.1\n",
      "Epoch: [3/10], Step: [1001/3125], Validation Acc: 68.1\n",
      "Epoch: [3/10], Step: [2001/3125], Validation Acc: 68.3\n",
      "Epoch: [3/10], Step: [3001/3125], Validation Acc: 70.6\n",
      "Epoch: [4/10], Step: [1001/3125], Validation Acc: 68.4\n",
      "Epoch: [4/10], Step: [2001/3125], Validation Acc: 71.9\n",
      "Epoch: [4/10], Step: [3001/3125], Validation Acc: 71.2\n",
      "Epoch: [5/10], Step: [1001/3125], Validation Acc: 70.5\n",
      "Epoch: [5/10], Step: [2001/3125], Validation Acc: 70.8\n",
      "Epoch: [5/10], Step: [3001/3125], Validation Acc: 72.6\n",
      "Epoch: [6/10], Step: [1001/3125], Validation Acc: 69.6\n",
      "Epoch: [6/10], Step: [2001/3125], Validation Acc: 72.7\n",
      "Epoch: [6/10], Step: [3001/3125], Validation Acc: 70.6\n",
      "Epoch: [7/10], Step: [1001/3125], Validation Acc: 70.1\n",
      "Epoch: [7/10], Step: [2001/3125], Validation Acc: 71.7\n",
      "Epoch: [7/10], Step: [3001/3125], Validation Acc: 72.7\n",
      "Epoch: [8/10], Step: [1001/3125], Validation Acc: 69.7\n",
      "Epoch: [8/10], Step: [2001/3125], Validation Acc: 71.1\n",
      "Epoch: [8/10], Step: [3001/3125], Validation Acc: 70.7\n",
      "Epoch: [9/10], Step: [1001/3125], Validation Acc: 71.0\n",
      "Epoch: [9/10], Step: [2001/3125], Validation Acc: 70.6\n",
      "Epoch: [9/10], Step: [3001/3125], Validation Acc: 71.7\n",
      "Epoch: [10/10], Step: [1001/3125], Validation Acc: 70.0\n",
      "Epoch: [10/10], Step: [2001/3125], Validation Acc: 70.2\n",
      "Epoch: [10/10], Step: [3001/3125], Validation Acc: 69.9\n",
      "end\n",
      "614.7463136\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data1,data2, lengths1,lengths2, labels in loader:\n",
    "        data_batch_1,data_batch_2, lengths_batch1,lengths_batch2, label_batch = data1,data2, lengths1, lengths2, labels \n",
    "        outputs = F.softmax(model(data_batch_1,data_batch_2, lengths_batch1,lengths_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "model_rnn = RNN(emb_size=300, hidden_size=100, num_layers=1, num_classes=3, vocab_size=50000)\n",
    "model_rnn.cuda()\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "print('start:\\n')\n",
    "print(time.clock() / 60)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data1, data2, lengths1, lengths2, labels) in enumerate(train_loader):\n",
    "        model_rnn.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model_rnn(data1,data2, lengths1, lengths2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model_rnn)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format(\n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "print('end')\n",
    "print(time.clock() / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_train_acc_mul_100 = Train_Acc\n",
    "rnn_val_acc_mul_100 = Val_Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_train_acc_concat_200 = Train_Acc\n",
    "cnn_val_acc_concat_200 = Val_Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "res = [cnn_train_acc_concat_200, cnn_val_acc_concat_200]\n",
    "csvfile = '/home/jupyter/cnn_concat_200.csv'\n",
    "with open(csvfile, \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    writer.writerows(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(rnn_train_acc_concat_200,'-o')\n",
    "#plt.legend(['validation'])\n",
    "plt.plot(rnn_val_acc_concat_200,'-o')\n",
    "plt.xlabel('training steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(('train','validation'))\n",
    "plt.title('RNN+hidden size 200+concatenation')\n",
    "\n",
    "plt.savefig('/home/jupyter/rnn_acc_concat_200.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_acc_concat_200 = pd.read_csv('/home/jupyter/rnn_concat_200.csv',header = None)\n",
    "rnn_acc_concat_200.T[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.7\n",
      "66.2\n",
      "68.9\n",
      "67.2\n",
      "66.8\n"
     ]
    }
   ],
   "source": [
    "print(max(cnn_val_acc_concat_10))\n",
    "print(max(cnn_val_acc_concat_20))\n",
    "print(max(cnn_val_acc_concat_50))\n",
    "print(max(cnn_val_acc_concat_100))\n",
    "print(max(cnn_val_acc_concat_500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.1\n",
      "68.3\n",
      "70.6\n",
      "70.0\n",
      "70.9\n",
      "70.8\n"
     ]
    }
   ],
   "source": [
    "print(max(rnn_val_acc_concat_10))\n",
    "print(max(rnn_val_acc_concat_20))\n",
    "print(max(rnn_val_acc_concat_50))\n",
    "print(max(rnn_val_acc_concat_100))\n",
    "print(max(rnn_val_acc_concat_200))\n",
    "print(max(rnn_val_acc_concat_500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imort MNLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_table('mnli_train.tsv')\n",
    "val = pd.read_table('mnli_val.tsv')\n",
    "\n",
    "#fiction_train = train.loc[train['genre']=='fiction'][['sentence1','sentence2','label']].values.tolist()\n",
    "#telephone_train = train.loc[train['genre']=='telephone'][['sentence1','sentence2','label']].values.tolist()\n",
    "#slate_train = train.loc[train['genre']=='slate'][['sentence1','sentence2','label']].values.tolist()\n",
    "#government_train = train.loc[train['genre']=='government'][['sentence1','sentence2','label']].values.tolist()\n",
    "#travel_train = train.loc[train['genre']=='travel'][['sentence1','sentence2','label']].values.tolist()\n",
    "\n",
    "fiction_val = val.loc[val['genre']=='fiction'][['sentence1','sentence2','label']].values.tolist()\n",
    "telephone_val = val.loc[val['genre']=='telephone'][['sentence1','sentence2','label']].values.tolist()\n",
    "slate_val = val.loc[val['genre']=='slate'][['sentence1','sentence2','label']].values.tolist()\n",
    "government_val = val.loc[val['genre']=='government'][['sentence1','sentence2','label']].values.tolist()\n",
    "travel_val = val.loc[val['genre']=='travel'][['sentence1','sentence2','label']].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize(fiction_train)\n",
    "tokenize(fiction_val)\n",
    "\n",
    "#tokenize(telephone_train)\n",
    "tokenize(telephone_val)\n",
    "\n",
    "#tokenize(slate_train)\n",
    "tokenize(slate_val)\n",
    "\n",
    "#tokenize(government_train)\n",
    "tokenize(government_val)\n",
    "\n",
    "#tokenize(travel_train)\n",
    "tokenize(travel_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fiction_train_id = token2id(fiction_train)\n",
    "fiction_val_id = token2id(fiction_val)\n",
    "\n",
    "#telephone_train_id = token2id(telephone_train)\n",
    "telephone_val_id = token2id(telephone_val)\n",
    "\n",
    "#slate_train_id = token2id(slate_train)\n",
    "slate_val_id = token2id(slate_val)\n",
    "\n",
    "#government_train_id = token2id(government_train)\n",
    "government_val_id = token2id(government_val)\n",
    "\n",
    "#travel_train_id = token2id(travel_train)\n",
    "travel_val_id = token2id(travel_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiction_val_dataset = myDataset(fiction_val_id)\n",
    "fiction_val_loader = torch.utils.data.DataLoader(dataset=fiction_val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=my_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "telephone_val_dataset = myDataset(telephone_val_id)\n",
    "telephone_val_loader = torch.utils.data.DataLoader(dataset=telephone_val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=my_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "slate_val_dataset = myDataset(slate_val_id)\n",
    "slate_val_loader = torch.utils.data.DataLoader(dataset=slate_val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=my_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "government_val_dataset = myDataset(government_val_id)\n",
    "government_val_loader = torch.utils.data.DataLoader(dataset=government_val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=my_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "travel_val_dataset = myDataset(travel_val_id)\n",
    "travel_val_loader = torch.utils.data.DataLoader(dataset=travel_val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=my_collate_func,\n",
    "                                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN\n",
      "Validation Accuracy for fiction: 44.824120603015075\n",
      "Validation Accuracy for telephone: 45.97014925373134\n",
      "Validation Accuracy for slate: 42.71457085828343\n",
      "Validation Accuracy for government: 43.30708661417323\n",
      "Validation Accuracy for travel: 45.11201629327902\n"
     ]
    }
   ],
   "source": [
    "print('CNN')\n",
    "print('Validation Accuracy for fiction: {}'.format(test_model(fiction_val_loader, model)))\n",
    "print('Validation Accuracy for telephone: {}'.format(test_model(telephone_val_loader, model)))\n",
    "print('Validation Accuracy for slate: {}'.format(test_model(slate_val_loader, model)))\n",
    "print('Validation Accuracy for government: {}'.format(test_model(government_val_loader, model)))\n",
    "print('Validation Accuracy for travel: {}'.format(test_model(travel_val_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN\n",
      "Validation Accuracy for fiction: 44.321608040201006\n",
      "Validation Accuracy for telephone: 40.69651741293532\n",
      "Validation Accuracy for slate: 41.91616766467066\n",
      "Validation Accuracy for government: 46.06299212598425\n",
      "Validation Accuracy for travel: 43.17718940936864\n"
     ]
    }
   ],
   "source": [
    "print('RNN')\n",
    "print('Validation Accuracy for fiction: {}'.format(test_model(fiction_val_loader, model)))\n",
    "print('Validation Accuracy for telephone: {}'.format(test_model(telephone_val_loader, model)))\n",
    "print('Validation Accuracy for slate: {}'.format(test_model(slate_val_loader, model)))\n",
    "print('Validation Accuracy for government: {}'.format(test_model(government_val_loader, model)))\n",
    "print('Validation Accuracy for travel: {}'.format(test_model(travel_val_loader, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_2(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i=0\n",
    "    pred = []\n",
    "    label = []\n",
    "    DATA1 = []\n",
    "    DATA2 = []\n",
    "    model.eval()\n",
    "    for data1, data2, lengths1, lengths2, labels in loader:\n",
    "        data_batch_1,data_batch_2, lengths_batch1,lengths_batch2, label_batch = data1,data2, lengths1, lengths2, labels\n",
    "        outputs = F.softmax(model(data_batch_1,data_batch_2, lengths_batch1,lengths_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        pred.append(predicted.view_as(labels))\n",
    "        DATA1.append(data1)\n",
    "        DATA2.append(data2)\n",
    "        label.append(labels)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return DATA1, DATA2, pred, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA1, DATA2, pred, label = test_model_2(val_loader,model_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: tensor([1, 2, 1, 0, 2, 2, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 2, 2, 0, 0, 0, 1, 0,\n",
      "        1, 1, 1, 2, 2, 1, 1, 1], device='cuda:0')\n",
      "prediction: tensor([2, 1, 1, 0, 2, 2, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 2, 0, 1, 2,\n",
      "        1, 1, 1, 1, 2, 1, 1, 1], device='cuda:0')\n",
      "equal: tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1], device='cuda:0', dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "i=20\n",
    "print('label: {}'.format(label[i]))\n",
    "print('prediction: {}'.format(pred[i]))\n",
    "print('equal: {}'.format(label[i] == pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a surfer is riding the waves whilst another surfer sits on his board waiting .\n",
      "A surfer watches as his friend is eaten by a large shark .\n",
      "true label: contradiction\n",
      "predicted label: neutral\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "example1 = DATA1[i][j]\n",
    "example1 = example1.tolist()\n",
    "example1 = list(filter(lambda a: a != 0, example1))\n",
    "example1_text = []\n",
    "for m in range(len(example1)):\n",
    "    example1_text.append(idx2words_ft[example1[m]])\n",
    "    \n",
    "example2 = DATA2[i][j]\n",
    "example2 = example2.tolist()\n",
    "example2 = list(filter(lambda a: a != 0, example2))\n",
    "example2_text = []\n",
    "for k in range(len(example2)):\n",
    "    example2_text.append(idx2words_ft[example2[k]])    \n",
    "\n",
    "    \n",
    "print(' '.join(example1_text))\n",
    "print(' '.join(example2_text))\n",
    "print('true label: {}'.format(list(label_dic.keys())[list(label_dic.values()).index(label[i][j])]))\n",
    "print('predicted label: {}'.format(list(label_dic.keys())[list(label_dic.values()).index(pred[i][j])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=1\n",
    "pred[i][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
